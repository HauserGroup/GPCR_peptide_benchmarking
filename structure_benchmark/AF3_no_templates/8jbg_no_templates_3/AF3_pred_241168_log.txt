>> Running the following command:
/var/spool/slurm//job241168/slurm_script ../structural_benchmark_without_templates.txt

>> Start time:  Wed Feb 26 08:42:28 CET 2025
   Machine:     ilfgridgpun03fl.unicph.domain
   Directory:   /projects/ilfgrid/people/pqh443/AF3/Scripts
   GPU:         0

>> AF3 input:
   .json:      /maps/projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/without_templates/8jbg_no_templates_3.json
>> Starting prediction for 8jbg_no_templates_3 


Launching AF3 with the following command:
python /projects/ilfgrid/apps/alphafold3/run_alphafold.py --db_dir /local_db/alphafold_db --run_data_pipeline=false --json_path /projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/without_templates/8jbg_no_templates_3.json --output_dir /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models --model_dir /local_db/alphafold3_model_parameters
I0226 08:42:30.550447 140498188694016 folding_input.py:1044] Detected /projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/without_templates/8jbg_no_templates_3.json is an AlphaFold 3 JSON since the top-level is not a list.
I0226 08:42:31.131630 140498188694016 xla_bridge.py:895] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0226 08:42:31.132715 140498188694016 xla_bridge.py:895] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0226 08:42:45.232476 140498188694016 pipeline.py:165] processing 8jbg_no_templates_3, random_seed=3
I0226 08:42:45.298882 140498188694016 pipeline.py:258] Calculating bucket size for input with 1438 tokens.
I0226 08:42:45.299034 140498188694016 pipeline.py:264] Got bucket size 1536 for input with 1438 tokens, resulting in 98 padded tokens.
2025-02-26 08:43:46.643975: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.284152343s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:43:49.216417: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.283987426s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:43:51.255736: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.016769531s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:44:13.252462: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.286315063s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:44:15.826450: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.285948364s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:44:17.857544: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.012954101s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:44:52.339462: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.223374878s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:44:54.790428: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.223812133s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:44:57.211263: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.209146362s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:44:59.412917: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.098143798s. {block_m:16,block_n:64,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:45:01.876216: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.230885864s. {block_m:16,block_n:128,block_k:32,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:45:32.166052: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.224071167s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:45:34.615582: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.223783447s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:45:37.036505: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.208671264s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:45:39.237357: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.098410034s. {block_m:16,block_n:64,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:45:41.699662: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.229141967s. {block_m:16,block_n:128,block_k:32,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
Running AlphaFold 3. Please note that standard AlphaFold 3 model parameters are
only available under terms of use provided at
https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md.
If you do not agree to these terms and are using AlphaFold 3 derived model
parameters, cancel execution of AlphaFold 3 inference with CTRL-C, and do not
use the model parameters.
Skipping running the data pipeline.
Found local devices: [CudaDevice(id=0)]
Building model from scratch...
Processing 1 fold inputs.
Processing fold input 8jbg_no_templates_3
Checking we can load the model parameters...
Skipping data pipeline...
Output directory: /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models/8jbg_no_templates_3
Writing model input JSON to /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models/8jbg_no_templates_3
Predicting 3D structure for 8jbg_no_templates_3 for seed(s) (3,)...
Featurising data for seeds (3,)...
Featurising 8jbg_no_templates_3 with rng_seed 3.
Featurising 8jbg_no_templates_3 with rng_seed 3 took 39.87 seconds.
Featurising data for seeds (3,) took  46.68 seconds.
Running model inference for seed 3...
Running model inference for seed 3 took  348.04 seconds.
Extracting output structures (one per sample) for seed 3...
Extracting output structures (one per sample) for seed 3 took  4.15 seconds.
Running model inference and extracting output structures for seed 3 took  352.19 seconds.
Running model inference and extracting output structures for seeds (3,) took  352.19 seconds.
Writing outputs for 8jbg_no_templates_3 for seed(s) (3,)...
Done processing fold input 8jbg_no_templates_3.
Done processing 1 fold inputs.
>> Prediction for 8jbg_no_templates_3 finished in 1740556167 seconds.


>> Job finished at: Wed Feb 26 08:49:27 CET 2025

>> Runtime: 419 s
