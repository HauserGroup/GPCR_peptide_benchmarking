>> Running the following command:
/var/spool/slurm//job241168/slurm_script ../structural_benchmark_without_templates.txt

>> Start time:  Wed Feb 26 08:49:27 CET 2025
   Machine:     ilfgridgpun03fl.unicph.domain
   Directory:   /projects/ilfgrid/people/pqh443/AF3/Scripts
   GPU:         0

>> AF3 input:
   .json:      /maps/projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/without_templates/8jbg_no_templates_4.json
>> Starting prediction for 8jbg_no_templates_4 


Launching AF3 with the following command:
python /projects/ilfgrid/apps/alphafold3/run_alphafold.py --db_dir /local_db/alphafold_db --run_data_pipeline=false --json_path /projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/without_templates/8jbg_no_templates_4.json --output_dir /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models --model_dir /local_db/alphafold3_model_parameters
I0226 08:49:29.950300 139717856121344 folding_input.py:1044] Detected /projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/without_templates/8jbg_no_templates_4.json is an AlphaFold 3 JSON since the top-level is not a list.
I0226 08:49:30.531840 139717856121344 xla_bridge.py:895] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0226 08:49:30.532977 139717856121344 xla_bridge.py:895] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0226 08:49:46.535679 139717856121344 pipeline.py:165] processing 8jbg_no_templates_4, random_seed=4
I0226 08:49:46.606339 139717856121344 pipeline.py:258] Calculating bucket size for input with 1438 tokens.
I0226 08:49:46.606477 139717856121344 pipeline.py:264] Got bucket size 1536 for input with 1438 tokens, resulting in 98 padded tokens.
2025-02-26 08:50:47.700338: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.219093505s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:50:50.142131: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.219408935s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:50:52.560419: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.208340454s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:50:54.759334: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.097419799s. {block_m:16,block_n:64,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:50:57.225448: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.231767578s. {block_m:16,block_n:128,block_k:32,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:51:14.717302: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.284360229s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:51:17.293574: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.286273071s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:51:19.325118: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.012963317s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:51:54.023190: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.224228881s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:51:56.477232: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.225484253s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:51:58.900635: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.209079834s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:52:01.103957: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.099624389s. {block_m:16,block_n:64,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:52:03.567631: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.229785034s. {block_m:16,block_n:128,block_k:32,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:52:21.373486: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.284069336s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-26 08:52:23.947259: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.28483435s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-26 08:52:25.972020: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.011475463s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
Running AlphaFold 3. Please note that standard AlphaFold 3 model parameters are
only available under terms of use provided at
https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md.
If you do not agree to these terms and are using AlphaFold 3 derived model
parameters, cancel execution of AlphaFold 3 inference with CTRL-C, and do not
use the model parameters.
Skipping running the data pipeline.
Found local devices: [CudaDevice(id=0)]
Building model from scratch...
Processing 1 fold inputs.
Processing fold input 8jbg_no_templates_4
Checking we can load the model parameters...
Skipping data pipeline...
Output directory: /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models/8jbg_no_templates_4
Writing model input JSON to /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models/8jbg_no_templates_4
Predicting 3D structure for 8jbg_no_templates_4 for seed(s) (4,)...
Featurising data for seeds (4,)...
Featurising 8jbg_no_templates_4 with rng_seed 4.
Featurising 8jbg_no_templates_4 with rng_seed 4 took 40.00 seconds.
Featurising data for seeds (4,) took  48.08 seconds.
Running model inference for seed 4...
Running model inference for seed 4 took  350.33 seconds.
Extracting output structures (one per sample) for seed 4...
Extracting output structures (one per sample) for seed 4 took  4.14 seconds.
Running model inference and extracting output structures for seed 4 took  354.47 seconds.
Running model inference and extracting output structures for seeds (4,) took  354.47 seconds.
Writing outputs for 8jbg_no_templates_4 for seed(s) (4,)...
Done processing fold input 8jbg_no_templates_4.
Done processing 1 fold inputs.
>> Prediction for 8jbg_no_templates_4 finished in 1740556592 seconds.


>> Job finished at: Wed Feb 26 08:56:32 CET 2025

>> Runtime: 425 s
