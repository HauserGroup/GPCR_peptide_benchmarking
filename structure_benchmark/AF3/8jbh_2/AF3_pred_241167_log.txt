>> Running the following command:
/var/spool/slurm//job241167/slurm_script ../structural_benchmark_with_templates.txt

>> Start time:  Tue Feb 25 21:01:26 CET 2025
   Machine:     ilfgridgpun01fl.unicph.domain
   Directory:   /projects/ilfgrid/people/pqh443/AF3/Scripts
   GPU:         1

>> AF3 input:
   .json:      /maps/projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/with_templates/8jbh_2.json
>> Starting prediction for 8jbh_2 


Launching AF3 with the following command:
python /projects/ilfgrid/apps/alphafold3/run_alphafold.py --db_dir /local_db/alphafold_db --run_data_pipeline=false --json_path /projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/with_templates/8jbh_2.json --output_dir /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models --model_dir /local_db/alphafold3_model_parameters
I0225 21:01:28.197355 139869230821888 folding_input.py:1044] Detected /projects/ilfgrid/people/pqh443/AF3/structural_benchmark_jsons/with_templates/8jbh_2.json is an AlphaFold 3 JSON since the top-level is not a list.
I0225 21:01:28.709481 139869230821888 xla_bridge.py:895] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0225 21:01:28.710701 139869230821888 xla_bridge.py:895] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0225 21:01:41.033276 139869230821888 pipeline.py:165] processing 8jbh_2, random_seed=2
I0225 21:01:41.118182 139869230821888 pipeline.py:258] Calculating bucket size for input with 1439 tokens.
I0225 21:01:41.118405 139869230821888 pipeline.py:264] Got bucket size 1536 for input with 1439 tokens, resulting in 97 padded tokens.
2025-02-25 21:02:48.168520: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.217449951s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:02:50.608355: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.218466797s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-25 21:02:53.028069: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.208114135s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:02:55.228498: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.099065307s. {block_m:16,block_n:64,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:02:57.688210: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1065_computation (parameter_0.87: bf16[1,1536,1536,128,1], parameter_1.87: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2315 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48883 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2315)
  %parameter_1.87 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2316 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.87), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48884 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2316)
  ROOT %dot.1672 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48883, bf16[128,1536,1536]{2,1,0} %bitcast.48884), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.227312133s. {block_m:16,block_n:128,block_k:32,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:03:15.208834: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.282498535s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:03:17.781239: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.283880981s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-25 21:03:19.823235: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1117_computation (parameter_0.15: bf16[1536,1536,128,1], parameter_1.15: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2293 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48748 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2293)
  %parameter_1.15 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2294 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.15), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48749 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2294)
  ROOT %dot.1600 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48748, bf16[128,1536,1536]{2,1,0} %bitcast.48749), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.020683288s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:04:07.477872: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.220999145s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:04:09.925135: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.222370361s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-25 21:04:12.347806: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.20920581s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:04:14.552381: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.10083374s. {block_m:16,block_n:64,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:04:17.013113: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1118_computation (parameter_0.17: bf16[1536,1536,128,1], parameter_1.17: bf16[1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(0)
  %transpose.2295 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_0.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48752 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2295)
  %parameter_1.17 = bf16[1536,1536,128,1]{3,2,1,0} parameter(1)
  %transpose.2296 = bf16[1,128,1536,1536]{3,2,1,0} transpose(bf16[1536,1536,128,1]{3,2,1,0} %parameter_1.17), dimensions={3,2,0,1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48753 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,128,1536,1536]{3,2,1,0} %transpose.2296)
  ROOT %dot.1602 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48752, bf16[128,1536,1536]{2,1,0} %bitcast.48753), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/evoformer/while/body/msa_stack/triangle_multiplication_incoming/ckj,cki->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.228426269s. {block_m:16,block_n:128,block_k:32,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:04:34.533081: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.284290527s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
2025-02-25 21:04:37.105998: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.286233032s. {block_m:16,block_n:16,block_k:256,split_k:1,num_stages:3,num_warps:4,num_ctas:1}
2025-02-25 21:04:39.148947: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1142] Slow kernel for %gemm_fusion_dot.1064_computation (parameter_0.85: bf16[1,1536,1536,128,1], parameter_1.85: bf16[1,1536,1536,128,1]) -> bf16[128,1536,1536] {
  %parameter_0.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(0)
  %transpose.2313 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_0.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48879 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2313)
  %parameter_1.85 = bf16[1,1536,1536,128,1]{4,3,2,1,0} parameter(1)
  %transpose.2314 = bf16[1,1,128,1536,1536]{4,3,2,1,0} transpose(bf16[1,1536,1536,128,1]{4,3,2,1,0} %parameter_1.85), dimensions={0,4,3,1,2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/slice" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
  %bitcast.48880 = bf16[128,1536,1536]{2,1,0} bitcast(bf16[1,1,128,1536,1536]{4,3,2,1,0} %transpose.2314)
  ROOT %dot.1670 = bf16[128,1536,1536]{2,1,0} dot(bf16[128,1536,1536]{2,1,0} %bitcast.48879, bf16[128,1536,1536]{2,1,0} %bitcast.48880), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={2}, metadata={op_name="jit(apply_fn)/jit(main)/diffuser/while/body/vmap(confidence_head)/while/body/confidence_pairformer/triangle_multiplication_outgoing/cik,cjk->cij/dot_general" source_file="/projects/ilfgrid/apps/alphafold3/run_alphafold.py" source_line=290}
} took: 1.020403686s. {block_m:16,block_n:64,block_k:128,split_k:1,num_stages:1,num_warps:4,num_ctas:1}
Running AlphaFold 3. Please note that standard AlphaFold 3 model parameters are
only available under terms of use provided at
https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md.
If you do not agree to these terms and are using AlphaFold 3 derived model
parameters, cancel execution of AlphaFold 3 inference with CTRL-C, and do not
use the model parameters.
Skipping running the data pipeline.
Found local devices: [CudaDevice(id=0)]
Building model from scratch...
Processing 1 fold inputs.
Processing fold input 8jbh_2
Checking we can load the model parameters...
Skipping data pipeline...
Output directory: /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models/8jbh_2
Writing model input JSON to /projects/ilfgrid/people/pqh443/AF3/Structural_Benchmark_Models/8jbh_2
Predicting 3D structure for 8jbh_2 for seed(s) (2,)...
Featurising data for seeds (2,)...
Featurising 8jbh_2 with rng_seed 2.
Featurising 8jbh_2 with rng_seed 2 took 43.15 seconds.
Featurising data for seeds (2,) took  49.79 seconds.
Running model inference for seed 2...
Running model inference for seed 2 took  348.34 seconds.
Extracting output structures (one per sample) for seed 2...
Extracting output structures (one per sample) for seed 2 took  3.08 seconds.
Running model inference and extracting output structures for seed 2 took  351.43 seconds.
Running model inference and extracting output structures for seeds (2,) took  351.43 seconds.
Writing outputs for 8jbh_2 for seed(s) (2,)...
Done processing fold input 8jbh_2.
Done processing 1 fold inputs.
>> Prediction for 8jbh_2 finished in 1740514107 seconds.


>> Job finished at: Tue Feb 25 21:08:27 CET 2025

>> Runtime: 421 s
